{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from random import sample\n",
    "import numpy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_map = {\n",
    "  \"codenet\": \"CodeNet\",\n",
    "  \"vl\": \"Plagiarism\",\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "  \"Gemini\": \"Gemini\",\n",
    "  \"deepseek\": \"Deepseek-R1:8b\",\n",
    "  \"codenet-chatgpt-1-codebert\": \"ChatGPT\",\n",
    "  \"vl-2016-chatgpt-codebert\": \"ChatGPT\"\n",
    "}\n",
    "\n",
    "embedder_map = {\n",
    "  \"codebert\": \"CodeBERT\",\n",
    "  \"Codebert\": \"CodeBERT\",\n",
    "  \"codet5p\": \"CodeT5+\",\n",
    "  \"Codebert:512\": \"CodeBERT\",\n",
    "}\n",
    "\n",
    "classifiers_map = {\n",
    "  \"codenet-deepseek\": \"CodeNet/Deepseek\",\n",
    "  \"codenet-Gemini\": \"CodeNet/Gemini\",\n",
    "  \"vl-deepseek\": \"Plagarism/Deepseek\",\n",
    "  \"vl-Gemini\": \"Plagarism/Gemini\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/stats/final-scores.csv', index_col=0)\n",
    "data_prev = pd.read_csv('../data/stats/score-apr-6.csv', index_col=0)\n",
    "data_chunked = pd.read_csv('../data/stats/apr-6-chunked.csv', index_col=0)\n",
    "data_embedding_size = pd.read_csv('../data/stats/embedding-length-result-on-fixed.csv', index_col=0)\n",
    "data_embedding_size['embedding_size'] = data_embedding_size['embedding'].apply(lambda x: int(x.split(':')[1]))\n",
    "data_embedding_size['embedding'] = data_embedding_size['embedding'].apply(lambda x: x.split(':')[0])\n",
    "\n",
    "data_across_llm = pd.read_csv('../data/stats/across-llm-final.csv', index_col=0)\n",
    "\n",
    "\n",
    "data_embedding_size_higher = pd.read_csv('../data/stats/embedding-length-result-final.csv', index_col=0)\n",
    "data_embedding_size_higher['embedding_size'] = data_embedding_size_higher['embedding'].apply(lambda x: int(x.split(':')[1]))\n",
    "data_embedding_size_higher['embedding'] = data_embedding_size_higher['embedding'].apply(lambda x: x.split(':')[0])\n",
    "\n",
    "def transformVals(dataset: pd.DataFrame):\n",
    "  dataset['dataset'] = dataset['dataset'].apply(lambda x: dataset_map[x])\n",
    "  dataset['llm'] = dataset['llm'].apply(lambda x: model_map[x] if x in model_map else x)\n",
    "  dataset['embedding'] = dataset['embedding'].apply(lambda x: embedder_map[x])\n",
    "  dataset['classifier'] = dataset['classifier'].apply(lambda x: classifiers_map[x])\n",
    "  return dataset\n",
    "\n",
    "data_embedding_size = transformVals(data_embedding_size)\n",
    "data_embedding_size_higher = transformVals(data_embedding_size_higher)\n",
    "data_across_llm = transformVals(data_across_llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRow(row):\n",
    "    row = row.strip().split(\"\\t\")\n",
    "    acc = float(row[3])\n",
    "    tpr = float(row[4])\n",
    "    tnr = float(row[5])\n",
    "    f1 = float(row[6])\n",
    "    return acc, tpr, tnr, f1\n",
    "\n",
    "def get_embedding(row):\n",
    "    row = row.strip().split(\"\\t\")\n",
    "    if row[2] in embedder_map:\n",
    "        return embedder_map[row[2]]\n",
    "    return row[2]\n",
    "\n",
    "def get_model(row):\n",
    "    row = row.strip().split(\"\\t\")\n",
    "    if row[1] in model_map:\n",
    "        return model_map[row[1]]\n",
    "    return row[1]\n",
    "\n",
    "def get_dataset(row):\n",
    "    row = row.strip().split(\"\\t\")\n",
    "    if row[0] in dataset_map:\n",
    "        return dataset_map[row[0]]\n",
    "    return row[0]\n",
    "\n",
    "def get_classifier(row):\n",
    "    row = row.strip().split(\"\\t\")\n",
    "    return row[7]\n",
    "\n",
    "def assembleLatexLine(dataset, languageModel, embedding, acc, tpr, tnr, f1, acc_2, tpr_2, tnr_2, f1_2):\n",
    "    assembledLine = f\"{dataset} & {languageModel} & {embedding} & {acc} & {tpr} & {tnr} & {f1} & {acc_2} & {tpr_2} & {tnr_2} & {f1_2} \\\\\\\\ \\n\"\n",
    "    return  assembledLine\n",
    "\n",
    "dataset_list = ['CodeNet', 'Plagiarism']\n",
    "languageModel_list = ['Gemini', 'Deepseek-R1']\n",
    "embedding_list = ['CodeBERT', 'CodeT5+']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_for_within = [[f\"{ds}-{llm}\",  ds, llm ] for ds in dataset_map.keys() for llm in model_map.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_decimals(val):\n",
    "  return f\"{val:.2f}\"\n",
    "\n",
    "def build_across_stats(score_df: pd.DataFrame):\n",
    "  result_str = {}\n",
    "  for clf, ds, llm in targets_for_within:\n",
    "    parsedRows = score_df[(score_df['classifier'] == clf) & ~(( score_df['dataset'] == ds) & (score_df['llm'] == llm))]\n",
    "    prasedRowsGB = parsedRows[parsedRows['model'] == 'GradientBoosted']\n",
    "    prasedRowsNN = parsedRows[parsedRows['model'] == 'NeuralNetwork']\n",
    "    result_str[clf] = \"\"\n",
    "    for row in prasedRowsGB.iterrows():\n",
    "      row = row[1]\n",
    "      nn_row = prasedRowsNN[(prasedRowsNN['classifier'] == row['classifier']) & (prasedRowsNN['embedding'] == row['embedding']) & (prasedRowsNN['dataset'] == row['dataset']) & (prasedRowsNN['llm'] == row['llm']) & (prasedRowsNN['model'] == 'NeuralNetwork')].iloc[0]\n",
    "      result_str[clf] += assembleLatexLine(\n",
    "        dataset_map[row['dataset']],\n",
    "        model_map[row['llm']],\n",
    "        embedder_map[row['embedding']], \n",
    "                 f\"\\\\textbf{{{limit_decimals(row['acc'])}}}\" if row['acc'] == prasedRowsGB['acc'].max() else limit_decimals(row['acc']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(row['tpr'])}}}\" if row['tpr'] == prasedRowsGB['tpr'].max() else limit_decimals(row['tpr']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(row['tnr'])}}}\" if row['tnr'] == prasedRowsGB['tnr'].max() else limit_decimals(row['tnr']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(row['f1'])}}}\" if row['f1'] == prasedRowsGB['f1'].max() else limit_decimals(row['f1']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(nn_row['acc'])}}}\" if nn_row['acc'] == prasedRowsNN['acc'].max() else limit_decimals(nn_row['acc']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(nn_row['tpr'])}}}\" if nn_row['tpr'] == prasedRowsNN['tpr'].max() else limit_decimals(nn_row['tpr']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(nn_row['tnr'])}}}\" if nn_row['tnr'] == prasedRowsNN['tnr'].max() else limit_decimals(nn_row['tnr']), \n",
    "                 f\"\\\\textbf{{{limit_decimals(nn_row['f1'])}}}\" if nn_row['f1'] == prasedRowsNN['f1'].max() else limit_decimals(nn_row['f1'])\n",
    "                 )\n",
    "  return result_str\n",
    "\n",
    "def build_within_stats(score_df: pd.DataFrame):\n",
    "  result_str_by_emb = {x: \"\" for x in embedder_map.keys()}\n",
    "  parsedRows = score_df[\n",
    "    score_df.apply(\n",
    "      lambda row: any(\n",
    "        (row['classifier'] == clf and row['dataset'] == ds and row['llm'] == llm)\n",
    "        for clf, ds, llm in targets_for_within\n",
    "      ),\n",
    "      axis=1\n",
    "    )\n",
    "  ]  \n",
    "  prasedRowsNN = parsedRows[parsedRows['model'] == 'NeuralNetwork']\n",
    "  prasedRowsGB = parsedRows[parsedRows['model'] == 'GradientBoosted']\n",
    "  for row in prasedRowsGB.iterrows():\n",
    "    row = row[1]\n",
    "    nn_row = prasedRowsNN[(prasedRowsNN['classifier'] == row['classifier']) & (prasedRowsNN['embedding'] == row['embedding']) & (prasedRowsNN['dataset'] == row['dataset']) & (prasedRowsNN['llm'] == row['llm'])].iloc[0]\n",
    "    result_str_by_emb[row['embedding']] += assembleLatexLine(\n",
    "      dataset_map[row['dataset']],\n",
    "      model_map[row['llm']],\n",
    "      embedder_map[row['embedding']], \n",
    "                f\"\\\\textbf{{{limit_decimals(row['acc'])}}}\" if row['acc'] == prasedRowsGB['acc'].max() else limit_decimals(row['acc']), \n",
    "                f\"\\\\textbf{{{limit_decimals(row['tpr'])}}}\" if row['tpr'] == prasedRowsGB['tpr'].max() else limit_decimals(row['tpr']), \n",
    "                f\"\\\\textbf{{{limit_decimals(row['tnr'])}}}\" if row['tnr'] == prasedRowsGB['tnr'].max() else limit_decimals(row['tnr']), \n",
    "                f\"\\\\textbf{{{limit_decimals(row['f1'])}}}\" if row['f1'] == prasedRowsGB['f1'].max() else limit_decimals(row['f1']), \n",
    "                f\"\\\\textbf{{{limit_decimals(nn_row['acc'])}}}\" if nn_row['acc'] == prasedRowsNN['acc'].max() else limit_decimals(nn_row['acc']), \n",
    "                f\"\\\\textbf{{{limit_decimals(nn_row['tpr'])}}}\" if nn_row['tpr'] == prasedRowsNN['tpr'].max() else limit_decimals(nn_row['tpr']), \n",
    "                f\"\\\\textbf{{{limit_decimals(nn_row['tnr'])}}}\" if nn_row['tnr'] == prasedRowsNN['tnr'].max() else limit_decimals(nn_row['tnr']), \n",
    "                f\"\\\\textbf{{{limit_decimals(nn_row['f1'])}}}\" if nn_row['f1'] == prasedRowsNN['f1'].max() else limit_decimals(nn_row['f1'])\n",
    "                )\n",
    "  return \"\".join(result_str_by_emb.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = data\n",
    "results_within = build_within_stats(use_df)\n",
    "results_across = build_across_stats(use_df)\n",
    "print(results_within)\n",
    "for clf, result_str in results_across.items():\n",
    "  print(f\"\\\\textbf{{{clf}}} \\n {result_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source'] = 'data'\n",
    "data_prev['source'] = 'data_prev'\n",
    "combined = pd.merge(data, data_prev, on=['dataset', 'llm', 'embedding', 'classifier', 'model'], suffixes=('_data', '_data_prev'))\n",
    "combined['acc_diff'] = combined['acc_data'] - combined['acc_data_prev']\n",
    "combined['tpr_diff'] = combined['tpr_data'] - combined['tpr_data_prev']\n",
    "combined['tnr_diff'] = combined['tnr_data'] - combined['tnr_data_prev']\n",
    "combined['f1_diff'] = combined['f1_data'] - combined['f1_data_prev']\n",
    "\n",
    "# Drop old data columns\n",
    "columns_to_drop = ['acc_data', 'tpr_data', 'tnr_data', 'f1_data', 'source_data', \n",
    "           'acc_data_prev', 'tpr_data_prev', 'tnr_data_prev', 'f1_data_prev', 'source_data_prev']\n",
    "combined.drop(columns=columns_to_drop, inplace=True)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acc\n",
    "val_col = 'acc'\n",
    "\n",
    "data_cpy = transformVals(data.copy(deep=True))\n",
    "data_cpy['row'] = data_cpy['dataset'] + '-' + data_cpy['llm']\n",
    "data_cpy['col'] = data_cpy['classifier']\n",
    "\n",
    "\n",
    "metric_2 = [\"CodeBERT\", \"CodeT5+\"]\n",
    "metric_1 = [\"NeuralNetwork\", \"GradientBoosted\"]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12), sharex=True, sharey=True)\n",
    "\n",
    "# Determine the global min and max for the color scale\n",
    "vmin = data_cpy[val_col].min()\n",
    "vmax = data_cpy[val_col].max()\n",
    "\n",
    "for i, metric1 in enumerate(metric_1):\n",
    "    for j, metric2 in enumerate(metric_2):\n",
    "\n",
    "        pivot = data_cpy.query(f'model == \"{metric1}\"').query(f'embedding == \"{metric2}\"').pivot(index='row', columns='col', values=val_col)\n",
    "        sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", cbar_kws={'label': 'Accuracy'}, ax=axs[i][j], vmin=vmin, vmax=vmax, cbar=j==1)\n",
    "\n",
    "        axs[i][j].set_title(f\"{metric2} Embedding in {metric1}\")\n",
    "        axs[i][j].set_xlabel('Classifier')\n",
    "        axs[i][j].set_ylabel('Dataset with LLM Code')\n",
    "        axs[i][j].set_xticklabels(pivot.index, rotation=80)\n",
    "\n",
    "\n",
    "plt.suptitle(f\"Accuracy Comparison of Classifiers on Different Embeddings\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_col = 'tnr'\n",
    "val_2_col = 'tpr'\n",
    "emb_len_data = data_embedding_size_higher.query('model == \"GradientBoosted\"')\n",
    "\n",
    "emb_len_data['row'] = emb_len_data['dataset'] + '-' + emb_len_data['llm']\n",
    "\n",
    "fig, axs = plt.subplots(2, len(classifiers_map), figsize=(17, 6), sharex=True, sharey=True)\n",
    "\n",
    "for i, (clf_key, clf_name) in enumerate(classifiers_map.items()):\n",
    "    clf_data = emb_len_data[emb_len_data['classifier'] == clf_name]\n",
    "    grouped = clf_data.groupby(['embedding_size', 'row'])[val_col].mean().unstack()\n",
    "    \n",
    "    # Sort the rows by embedding size numerically\n",
    "    grouped = grouped.sort_index(ascending=True)\n",
    "    \n",
    "    grouped.plot(kind='line', ax=axs[0][i], marker='o', legend=False)\n",
    "    axs[0][i].set_title(f'Classifier: {clf_name}')\n",
    "    axs[0][i].set_xlabel('Embedding Size')\n",
    "    axs[0][i].set_ylabel('TNR')\n",
    "\n",
    "\n",
    "    grouped2 = clf_data.groupby(['embedding_size', 'row'])[val_2_col].mean().unstack()\n",
    "    \n",
    "    # Sort the rows by embedding size numerically\n",
    "    grouped2 = grouped2.sort_index(ascending=True)\n",
    "\n",
    "    grouped2.plot(kind='line', ax=axs[1][i], marker='o')\n",
    "    axs[1][i].set_xlabel('Embedding Sequence Length')\n",
    "    axs[1][i].set_ylabel('TPR')\n",
    "    axs[1][i].legend(title='Dataset and LLM')\n",
    "\n",
    "    # Adjust the y-axis to not start at 0\n",
    "    axs[0][i].set_ylim(0, 1.05)\n",
    "\n",
    "plt.suptitle(f\"TPR and TNR Comparison of Classifiers on Different Embedding Sequence Sizes, for Neural Network\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_col = 'tnr'\n",
    "emb_len_data = data_across_llm.query('model == \"NeuralNetwork\"')\n",
    "\n",
    "emb_len_data['row'] = emb_len_data['dataset'] + '-' + emb_len_data['llm']\n",
    "\n",
    "# Group data by 'row' and calculate the mean of the specified column\n",
    "grouped = emb_len_data.groupby(['classifier', 'row'])[val_col].mean().unstack()\n",
    "\n",
    "# Create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "grouped.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title(f'TNR across Classifiers and Datasets with LLM Code, for Neural Network Classifiers')\n",
    "ax.set_xlabel('Classifier')\n",
    "ax.set_ylabel('True Negative Rate (TNR)')\n",
    "ax.set_xticklabels(grouped.index, rotation=0)\n",
    "ax.legend(title='Dataset with LLM Code', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust the y-axis to not start at 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_col = 'tnr'\n",
    "emb_len_data = data.query('model == \"NeuralNetwork\"')\n",
    "\n",
    "emb_len_data['row'] = emb_len_data['dataset'] + '-' + emb_len_data['llm']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Group data by classifier and embedding size\n",
    "grouped = emb_len_data.groupby(['classifier', 'tpr', 'dataset'])[val_col].mean().unstack()\n",
    "\n",
    "# Create a grouped bar chart\n",
    "grouped.plot(kind='bar', ax=ax, width=0.8)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('True Negative Rate (TNR) by Embedding Size and Classifier')\n",
    "ax.set_xlabel('Embedding Size')\n",
    "ax.set_ylabel('True Negative Rate (TNR)')\n",
    "ax.legend(title='Classifier', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
